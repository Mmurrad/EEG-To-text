corpus_bleu_score = [0.12687238829958217, 0.01297725529270703, 1.5532333588608222e-104, 1.6992748896801848e-155]
sacre_blue_score = {'score': 0.08833680785068952, 'counts': [7357, 109, 11, 0], 'totals': [53692, 52494, 51296, 50098], 'precisions': [13.702227519928481, 0.20764277822227303, 0.021444167186525264, 0.000998043834085193], 'bp': 1.0, 'sys_len': 53692, 'ref_len': 25575}
rouge_scores = {'rouge-1': {'r': 0.2031687123790721, 'p': 0.08879638579343667, 'f': 0.11870915302024634}, 'rouge-2': {'r': 0.0022226939159401553, 'p': 0.0011144125999115377, 'f': 0.0014292104564891978}, 'rouge-l': {'r': 0.14720543999677246, 'p': 0.061291569337431864, 'f': 0.08300546878874822}}
wer_scores = 2.139030553641167
cer_scores = 1.3765068340542574
corpus_bleu_score_with_tf = [0, 0, 0, 0]
sacre_blue_score_with_tf = {'score': 0.0, 'counts': [0, 0, 0, 0], 'totals': [0, 0, 0, 0], 'precisions': [0.0, 0.0, 0.0, 0.0], 'bp': 0.0, 'sys_len': 0, 'ref_len': 25575}
rouge_scores_with_tf = Hypothesis is empty
wer_scores_with_tf = 1.0
cer_scores_with_tf = 1.0
