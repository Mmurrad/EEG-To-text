corpus_bleu_score = [0.12687238829958217, 0.01297725529270703, 1.5532333588608222e-104, 1.6992748896801848e-155]
sacre_blue_score = {'score': 0.08833680785068952, 'counts': [7357, 109, 11, 0], 'totals': [53692, 52494, 51296, 50098], 'precisions': [13.702227519928481, 0.20764277822227303, 0.021444167186525264, 0.000998043834085193], 'bp': 1.0, 'sys_len': 53692, 'ref_len': 25575}
rouge_scores = {'rouge-1': {'r': 0.2031687123790721, 'p': 0.08879638579343667, 'f': 0.11870915302024634}, 'rouge-2': {'r': 0.0022226939159401553, 'p': 0.0011144125999115377, 'f': 0.0014292104564891978}, 'rouge-l': {'r': 0.14720543999677246, 'p': 0.061291569337431864, 'f': 0.08300546878874822}}
wer_scores = 2.139030553641167
cer_scores = 1.3765068340542574
corpus_bleu_score_with_tf = [0, 0, 0, 0]
sacre_blue_score_with_tf = {'score': 0.0, 'counts': [0, 0, 0, 0], 'totals': [0, 0, 0, 0], 'precisions': [0.0, 0.0, 0.0, 0.0], 'bp': 0.0, 'sys_len': 0, 'ref_len': 25575}
rouge_scores_with_tf = Hypothesis is empty
wer_scores_with_tf = 1.0
cer_scores_with_tf = 1.0
corpus_bleu_score = [0.10641013055855501, 0.009514730056812977, 1.2629295492761852e-104, 1.4550249867328268e-155]
sacre_blue_score = {'score': 0.09368988813142644, 'counts': [2258, 60, 1, 0], 'totals': [17813, 17421, 17029, 16637], 'precisions': [12.676135406725425, 0.3444119166523162, 0.005872335427799636, 0.003005349522149426], 'bp': 1.0, 'sys_len': 17813, 'ref_len': 7236}
rouge_scores = {'rouge-1': {'r': 0.1818963963744217, 'p': 0.06837877537319165, 'f': 0.09571953084425852}, 'rouge-2': {'r': 0.001946904240623452, 'p': 0.0007170240006522786, 'f': 0.0010179056528060633}, 'rouge-l': {'r': 0.13730354127072641, 'p': 0.048955413015379805, 'f': 0.06947652749195231}}
wer_scores = 2.5716700473292766
cer_scores = 1.9295200638394894
corpus_bleu_score_with_tf = [0, 0, 0, 0]
sacre_blue_score_with_tf = {'score': 0.0, 'counts': [0, 0, 0, 0], 'totals': [0, 0, 0, 0], 'precisions': [0.0, 0.0, 0.0, 0.0], 'bp': 0.0, 'sys_len': 0, 'ref_len': 7236}
rouge_scores_with_tf = Hypothesis is empty
wer_scores_with_tf = 1.0
cer_scores_with_tf = 1.0
corpus_bleu_score = [0.11837994694596636, 0.02217398156704092, 0.003907137250712786, 1.7633725683721825e-79]
sacre_blue_score = {'score': 0.04956793484681359, 'counts': [4150, 91, 0, 0], 'totals': [23752, 22341, 20930, 19519], 'precisions': [17.472212866284945, 0.4073228593169509, 0.0023889154323936935, 0.001280803319842205], 'bp': 0.7256184989441999, 'sys_len': 23752, 'ref_len': 31370}
rouge_scores = {'rouge-1': {'r': 0.07696966687474191, 'p': 0.08863989848019485, 'f': 0.07768277730865544}, 'rouge-2': {'r': 0.002258817177550303, 'p': 0.0021819996829705503, 'f': 0.002045885976726947}, 'rouge-l': {'r': 0.06831630688856906, 'p': 0.07808464238800107, 'f': 0.06867297231743094}}
wer_scores = 1.0529154893398407
cer_scores = 0.8125811066110564
corpus_bleu_score_with_tf = [0.3536463813057201, 0.17778575815697428, 0.08812768853708489, 0.03980926365765694]
sacre_blue_score_with_tf = {'score': 3.165642040250981, 'counts': [10601, 2339, 522, 56], 'totals': [30213, 28802, 27391, 25980], 'precisions': [35.08754509648165, 8.120963821956808, 1.9057354605527363, 0.2155504234026174], 'bp': 0.962429200187127, 'sys_len': 30213, 'ref_len': 31370}
rouge_scores_with_tf = {'rouge-1': {'r': 0.23737673222877553, 'p': 0.292937621148485, 'f': 0.2606580525850563}, 'rouge-2': {'r': 0.056826332575883626, 'p': 0.06532113726100022, 'f': 0.06060050477481818}, 'rouge-l': {'r': 0.22194848263634342, 'p': 0.27262024376213595, 'f': 0.24326636463822898}}
wer_scores_with_tf = 0.8030530989688451
cer_scores_with_tf = 0.6420022091177412
