corpus_bleu_score = [0.13814995873277888, 0.016103384562029376, 0.0020455794191161216, 1.1747578804219639e-79]
sacre_blue_score = {'score': 0.06252888686514045, 'counts': [8266, 157, 2, 0], 'totals': [55804, 54598, 53392, 52186], 'precisions': [14.81255823955272, 0.2875563207443496, 0.0037458795325142342, 0.0009581113708657494], 'bp': 1.0, 'sys_len': 55804, 'ref_len': 28005}
rouge_scores = {'rouge-1': {'r': 0.24429432725162814, 'p': 0.10301023304243387, 'f': 0.14023424355361128}, 'rouge-2': {'r': 0.0034094766348735078, 'p': 0.0014426683005323813, 'f': 0.0019355368532201643}, 'rouge-l': {'r': 0.18082383399963384, 'p': 0.07435484824528697, 'f': 0.10203056125993117}}
wer_scores = 2.011910234443562
cer_scores = 1.3840247040658775
corpus_bleu_score_with_tf = [0, 0, 0, 0]
sacre_blue_score_with_tf = {'score': 0.0, 'counts': [0, 0, 0, 0], 'totals': [0, 0, 0, 0], 'precisions': [0.0, 0.0, 0.0, 0.0], 'bp': 0.0, 'sys_len': 0, 'ref_len': 28005}
rouge_scores_with_tf = Hypothesis is empty
wer_scores_with_tf = 1.0
cer_scores_with_tf = 1.0
